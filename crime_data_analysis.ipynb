{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Data Analysis Project\n",
    "\n",
    "Welcome! This notebook walks you through a full crime data analysis pipeline, from raw data processing to model development and dashboard creation.\n",
    "\n",
    "Each section starts with a brief explanation, followed by code and results. Comments in code cells clarify each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing: Convert Raw CSV to Parquet\n",
    "\n",
    "The raw CSV file is large. We process it in chunks, optimize data types, and save as Parquet for faster loading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:43,474 - INFO - Processing chunk 1\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:44,174 - INFO - Processing chunk 2\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:44,858 - INFO - Processing chunk 3\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:45,530 - INFO - Processing chunk 4\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:46,240 - INFO - Processing chunk 5\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:46,866 - INFO - Processing chunk 6\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:47,538 - INFO - Processing chunk 7\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:48,499 - INFO - Processing chunk 8\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:49,242 - INFO - Processing chunk 9\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:50,026 - INFO - Processing chunk 10\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "/tmp/ipykernel_30647/1563075441.py:42: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  for i, chunk in enumerate(chunk_iterator):\n",
      "2025-08-06 19:53:50,122 - INFO - Processing chunk 11\n",
      "2025-08-06 19:53:53,829 - INFO - Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Load raw CSV in chunks, optimize types, and save as Parquet\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import os as os\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def process_crime_data(input_path, output_path):\n",
    "    # Define column types for memory efficiency\n",
    "    dtype_mapping = {\n",
    "        'DR_NO': 'int64',\n",
    "        'AREA': 'int16',\n",
    "        'Rpt Dist No': 'int16',\n",
    "        'Part 1-2': 'int8',\n",
    "        'Crm Cd': 'int16',\n",
    "        'Vict Age': 'int8',\n",
    "        'Vict Sex': 'category',\n",
    "        'Vict Descent': 'category',\n",
    "        'Premis Cd': 'float32',\n",
    "        'Weapon Used Cd': 'float32',\n",
    "        'Status': 'category',\n",
    "        'Crm Cd 1': 'float32',\n",
    "        'Crm Cd 2': 'float32',\n",
    "        'Crm Cd 3': 'float32',\n",
    "        'Crm Cd 4': 'float32',\n",
    "        'LAT': 'float32',\n",
    "        'LON': 'float32'\n",
    "    }\n",
    "    date_columns = ['Date Rptd', 'DATE OCC']\n",
    "    try:\n",
    "        chunk_iterator = pd.read_csv(\n",
    "            input_path,\n",
    "            chunksize=100000,\n",
    "            dtype=dtype_mapping,\n",
    "            parse_dates=date_columns,\n",
    "            na_values=['', 'NA', 'N/A']\n",
    "        )\n",
    "        processed_chunks = []\n",
    "        for i, chunk in enumerate(chunk_iterator):\n",
    "            logging.info(f'Processing chunk {i+1}')\n",
    "            processed_chunks.append(chunk)\n",
    "        df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        df.to_parquet(output_path, index=False)\n",
    "        logging.info('Data processing complete.')\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error during data processing: {e}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_file = Path('/home/ayyan/ANALYSIS_PROJECT/Crime_Data_from_2020_to_Present.csv')\n",
    "    output_file = Path('/home/ayyan/ANALYSIS_PROJECT/processed_crime_data.parquet')\n",
    "    process_crime_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Complete\n",
    "\n",
    "The raw CSV is now converted to Parquet. Next, let's understand the processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding\n",
    "\n",
    "Let's load the processed data and check its structure, missing values, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1004991 entries, 0 to 1004990\n",
      "Data columns (total 28 columns):\n",
      " #   Column          Non-Null Count    Dtype         \n",
      "---  ------          --------------    -----         \n",
      " 0   DR_NO           1004991 non-null  int64         \n",
      " 1   Date Rptd       1004991 non-null  datetime64[ns]\n",
      " 2   DATE OCC        1004991 non-null  datetime64[ns]\n",
      " 3   TIME OCC        1004991 non-null  int64         \n",
      " 4   AREA            1004991 non-null  int16         \n",
      " 5   AREA NAME       1004991 non-null  object        \n",
      " 6   Rpt Dist No     1004991 non-null  int16         \n",
      " 7   Part 1-2        1004991 non-null  int8          \n",
      " 8   Crm Cd          1004991 non-null  int16         \n",
      " 9   Crm Cd Desc     1004991 non-null  object        \n",
      " 10  Mocodes         853372 non-null   object        \n",
      " 11  Vict Age        1004991 non-null  int8          \n",
      " 12  Vict Sex        860347 non-null   object        \n",
      " 13  Vict Descent    860335 non-null   object        \n",
      " 14  Premis Cd       1004975 non-null  float32       \n",
      " 15  Premis Desc     1004403 non-null  object        \n",
      " 16  Weapon Used Cd  327247 non-null   float32       \n",
      " 17  Weapon Desc     327247 non-null   object        \n",
      " 18  Status          1004990 non-null  object        \n",
      " 19  Status Desc     1004991 non-null  object        \n",
      " 20  Crm Cd 1        1004980 non-null  float32       \n",
      " 21  Crm Cd 2        69160 non-null    float32       \n",
      " 22  Crm Cd 3        2314 non-null     float32       \n",
      " 23  Crm Cd 4        64 non-null       float32       \n",
      " 24  LOCATION        1004991 non-null  object        \n",
      " 25  Cross Street    154236 non-null   object        \n",
      " 26  LAT             1004991 non-null  float32       \n",
      " 27  LON             1004991 non-null  float32       \n",
      "dtypes: datetime64[ns](2), float32(8), int16(3), int64(2), int8(2), object(11)\n",
      "memory usage: 153.3+ MB\n",
      "\n",
      "Missing Value Counts:\n",
      "DR_NO                   0\n",
      "Date Rptd               0\n",
      "DATE OCC                0\n",
      "TIME OCC                0\n",
      "AREA                    0\n",
      "AREA NAME               0\n",
      "Rpt Dist No             0\n",
      "Part 1-2                0\n",
      "Crm Cd                  0\n",
      "Crm Cd Desc             0\n",
      "Mocodes            151619\n",
      "Vict Age                0\n",
      "Vict Sex           144644\n",
      "Vict Descent       144656\n",
      "Premis Cd              16\n",
      "Premis Desc           588\n",
      "Weapon Used Cd     677744\n",
      "Weapon Desc        677744\n",
      "Status                  1\n",
      "Status Desc             0\n",
      "Crm Cd 1               11\n",
      "Crm Cd 2           935831\n",
      "Crm Cd 3          1002677\n",
      "Crm Cd 4          1004927\n",
      "LOCATION                0\n",
      "Cross Street       850755\n",
      "LAT                     0\n",
      "LON                     0\n",
      "dtype: int64\n",
      "\n",
      "Summary Statistics:\n",
      "              DR_NO                      Date Rptd  \\\n",
      "count  1.004991e+06                        1004991   \n",
      "mean   2.202215e+08  2022-05-25 22:02:46.977017856   \n",
      "min    8.170000e+02            2020-01-01 00:00:00   \n",
      "25%    2.106169e+08            2021-04-21 00:00:00   \n",
      "50%    2.209159e+08            2022-06-08 00:00:00   \n",
      "75%    2.311103e+08            2023-07-04 00:00:00   \n",
      "max    2.521041e+08            2025-06-04 00:00:00   \n",
      "std    1.319718e+07                            NaN   \n",
      "\n",
      "                            DATE OCC      TIME OCC          AREA  \\\n",
      "count                        1004991  1.004991e+06  1.004991e+06   \n",
      "mean   2022-05-13 17:50:37.556754176  1.339900e+03  1.069174e+01   \n",
      "min              2020-01-01 00:00:00  1.000000e+00  1.000000e+00   \n",
      "25%              2021-04-06 00:00:00  9.000000e+02  5.000000e+00   \n",
      "50%              2022-05-26 00:00:00  1.420000e+03  1.100000e+01   \n",
      "75%              2023-06-22 00:00:00  1.900000e+03  1.600000e+01   \n",
      "max              2025-05-29 00:00:00  2.359000e+03  2.100000e+01   \n",
      "std                              NaN  6.510613e+02  6.110255e+00   \n",
      "\n",
      "        Rpt Dist No      Part 1-2        Crm Cd      Vict Age     Premis Cd  \\\n",
      "count  1.004991e+06  1.004991e+06  1.004991e+06  1.004991e+06  1.004975e+06   \n",
      "mean   1.115633e+03  1.400348e+00  5.001568e+02  2.891706e+01  3.056202e+02   \n",
      "min    1.010000e+02  1.000000e+00  1.100000e+02 -4.000000e+00  1.010000e+02   \n",
      "25%    5.870000e+02  1.000000e+00  3.310000e+02  0.000000e+00  1.010000e+02   \n",
      "50%    1.139000e+03  1.000000e+00  4.420000e+02  3.000000e+01  2.030000e+02   \n",
      "75%    1.613000e+03  2.000000e+00  6.260000e+02  4.400000e+01  5.010000e+02   \n",
      "max    2.199000e+03  2.000000e+00  9.560000e+02  1.200000e+02  9.760000e+02   \n",
      "std    6.111605e+02  4.899691e-01  2.052731e+02  2.199272e+01  2.193021e+02   \n",
      "\n",
      "       Weapon Used Cd      Crm Cd 1      Crm Cd 2     Crm Cd 3    Crm Cd 4  \\\n",
      "count   327247.000000  1.004980e+06  69160.000000  2314.000000   64.000000   \n",
      "mean       363.955353  4.999174e+02    958.101257   984.015991  991.218750   \n",
      "min        101.000000  1.100000e+02    210.000000   310.000000  821.000000   \n",
      "25%        311.000000  3.310000e+02    998.000000   998.000000  998.000000   \n",
      "50%        400.000000  4.420000e+02    998.000000   998.000000  998.000000   \n",
      "75%        400.000000  6.260000e+02    998.000000   998.000000  998.000000   \n",
      "max        516.000000  9.560000e+02    999.000000   999.000000  999.000000   \n",
      "std        123.734528  2.050736e+02    110.354347    52.350983   27.069851   \n",
      "\n",
      "                LAT           LON  \n",
      "count  1.004991e+06  1.004991e+06  \n",
      "mean   3.399821e+01 -1.180909e+02  \n",
      "min    0.000000e+00 -1.186676e+02  \n",
      "25%    3.401470e+01 -1.184305e+02  \n",
      "50%    3.405890e+01 -1.183225e+02  \n",
      "75%    3.416490e+01 -1.182739e+02  \n",
      "max    3.433430e+01  0.000000e+00  \n",
      "std    1.610713e+00  5.582386e+00  \n",
      "\n",
      "Target Variable:\n",
      "We'll use 'Crm Cd Desc' (Crime Code Description) as the target for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Load processed Parquet and display info, missing values, and stats\n",
    "def data_understanding(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    print('DataFrame Info:')\n",
    "    df.info()\n",
    "    print('\\nMissing Value Counts:')\n",
    "    print(df.isnull().sum())\n",
    "    print('\\nSummary Statistics:')\n",
    "    print(df.describe())\n",
    "    print('\\nTarget Variable:')\n",
    "    print(\"We'll use 'Crm Cd Desc' (Crime Code Description) as the target for prediction.\")\n",
    "    return df\n",
    "\n",
    "df_processed = data_understanding('/home/ayyan/ANALYSIS_PROJECT/processed_crime_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding Summary\n",
    "\n",
    "We now know the data types, missing values, and basic statistics. Next, we clean and engineer features for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We handle missing values, engineer time features, and remove outliers to prepare the data for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30647/541413313.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Premis Cd'].fillna(-1, inplace=True)\n",
      "/tmp/ipykernel_30647/541413313.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Crm Cd 1'].fillna(-1, inplace=True)\n",
      "/tmp/ipykernel_30647/541413313.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Weapon Used Cd'].fillna(-1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Clean data, engineer features, and save preprocessed Parquet\n",
    "def preprocess_data(df, output_path):\n",
    "    # Fill missing categorical values with 'Unknown'\n",
    "    categorical_cols = ['Vict Sex', 'Vict Descent', 'Weapon Desc', 'Premis Desc', 'Status']\n",
    "    for col in categorical_cols:\n",
    "        if df[col].dtype == 'object':\n",
    "            df[col] = df[col].astype('category')\n",
    "        if \"Unknown\" not in df[col].cat.categories:\n",
    "            df[col] = df[col].cat.add_categories(\"Unknown\")\n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "    # Fill missing numerical values with -1\n",
    "    df['Premis Cd'].fillna(-1, inplace=True)\n",
    "    df['Crm Cd 1'].fillna(-1, inplace=True)\n",
    "    df['Weapon Used Cd'].fillna(-1, inplace=True)\n",
    "    # Drop columns with many missing values\n",
    "    df.drop(columns=['Mocodes', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'Cross Street'], inplace=True)\n",
    "    # Feature engineering: extract hour, day, month\n",
    "    df['hour_of_day'] = df['DATE OCC'].dt.hour\n",
    "    df['day_of_week'] = df['DATE OCC'].dt.day_name().astype('category')\n",
    "    df['month'] = df['DATE OCC'].dt.month_name().astype('category')\n",
    "    # Outlier handling: cap victim age, remove invalid lat/lon\n",
    "    df['Vict Age'] = np.where((df['Vict Age'] > 100) | (df['Vict Age'] <= 0), df['Vict Age'].median(), df['Vict Age'])\n",
    "    df = df[(df['LAT'] != 0) & (df['LON'] != 0)].copy()\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    print('Preprocessing complete.')\n",
    "    return df\n",
    "\n",
    "df_preprocessed = preprocess_data(df_processed.copy(), '/home/ayyan/ANALYSIS_PROJECT/preprocessed_crime_data.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Complete\n",
    "\n",
    "The data is now clean and ready for analysis. Let's visualize key patterns next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "We visualize distributions and relationships to uncover patterns in the crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30647/1538345887.py:13: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(y=df['AREA NAME'], order=area_order, palette='viridis')\n",
      "2025-08-06 19:54:08,814 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "/tmp/ipykernel_30647/1538345887.py:20: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=df['day_of_week'], order=day_order, palette='magma')\n",
      "2025-08-06 19:54:09,416 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-06 19:54:10,481 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "/tmp/ipykernel_30647/1538345887.py:26: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=df['month'], order=month_order, palette='cividis')\n",
      "2025-08-06 19:54:11,119 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "2025-08-06 19:54:12,536 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n",
      "/tmp/ipykernel_30647/1538345887.py:33: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=df['hour_of_day'], palette='plasma')\n",
      "2025-08-06 19:54:13,126 - INFO - Using categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA visualizations saved.\n"
     ]
    }
   ],
   "source": [
    "# Save EDA plots for age, area, time, and correlations\n",
    "def perform_eda(df, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    sns.set_style('whitegrid')\n",
    "    # Victim Age Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['Vict Age'], bins=30, kde=True)\n",
    "    plt.title('Distribution of Victim Age')\n",
    "    plt.savefig(os.path.join(output_dir, 'victim_age_distribution.png')); plt.close()\n",
    "    # Crimes by Area Name (Top 15)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    area_order = df['AREA NAME'].value_counts().iloc[:15].index\n",
    "    sns.countplot(y=df['AREA NAME'], order=area_order, palette='viridis')\n",
    "    plt.title('Top 15 Areas by Number of Crimes')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'crimes_by_area.png')); plt.close()\n",
    "    # Crimes by Day of Week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    sns.countplot(x=df['day_of_week'], order=day_order, palette='magma')\n",
    "    plt.title('Number of Crimes by Day of the Week')\n",
    "    plt.savefig(os.path.join(output_dir, 'crimes_by_day_of_week.png')); plt.close()\n",
    "    # Crimes by Month\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    sns.countplot(x=df['month'], order=month_order, palette='cividis')\n",
    "    plt.title('Number of Crimes by Month')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'crimes_by_month.png')); plt.close()\n",
    "    # Crimes by Hour of Day\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.countplot(x=df['hour_of_day'], palette='plasma')\n",
    "    plt.title('Number of Crimes by Hour of the Day')\n",
    "    plt.savefig(os.path.join(output_dir, 'crimes_by_hour.png')); plt.close()\n",
    "    # Correlation Heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "    plt.title('Correlation Matrix of Numerical Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'correlation_heatmap.png')); plt.close()\n",
    "    print('EDA visualizations saved.')\n",
    "\n",
    "perform_eda(df_preprocessed, '/home/ayyan/ANALYSIS_PROJECT/eda_plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Complete\n",
    "\n",
    "Key visualizations are saved. Review them to spot trends in age, area, time, and feature correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Development\n",
    "\n",
    "We build and compare several models to predict crime type. Each step is commented for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Grid Search...\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayyan/jupyter_venv/jupyter_venv/lib/python3.13/site-packages/sklearn/model_selection/_split.py:811: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# IMPROVED MODEL DEVELOPMENT - RANDOM FOREST FOCUS\n",
    "# =============================================\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "\n",
    "# Filter out warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# 1. Prepare features and target\n",
    "# Filter out rare crime categories (those with < 5 instances)\n",
    "crime_counts = df_preprocessed['Crm Cd Desc'].value_counts()\n",
    "rare_crimes = crime_counts[crime_counts < 5].index\n",
    "df_filtered = df_preprocessed[~df_preprocessed['Crm Cd Desc'].isin(rare_crimes)]\n",
    "\n",
    "# Define features and target - focus on most predictive features\n",
    "X = df_filtered[[\n",
    "    'AREA', 'Rpt Dist No', 'Part 1-2', 'Vict Age', 'Vict Sex', \n",
    "    'Vict Descent', 'hour_of_day', 'day_of_week', 'month', 'LAT', 'LON'\n",
    "]]\n",
    "y = df_filtered['Crm Cd Desc']\n",
    "\n",
    "# 2. Encode target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# 3. Identify feature types\n",
    "categorical_features = ['Vict Sex', 'Vict Descent', 'day_of_week', 'month']\n",
    "numerical_features = ['AREA', 'Rpt Dist No', 'Part 1-2', 'Vict Age', 'hour_of_day', 'LAT', 'LON']\n",
    "\n",
    "# 4. Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "    ('num', StandardScaler(), numerical_features)\n",
    "])\n",
    "\n",
    "# 5. Enhanced train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# 6. Optimized Random Forest Pipeline\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        class_weight='balanced',  # Handle class imbalance\n",
    "        n_jobs=-1  # Use all cores\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 7. Comprehensive hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [None, 15, 25, 35],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_grid,\n",
    "    cv=5,  # More folds for better validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Grid Search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 8. Best model evaluation\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print('\\nBest Parameters:')\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print('\\nModel Performance:')\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, y_pred,\n",
    "    target_names=label_encoder.classes_,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# 9. Feature Importance Analysis\n",
    "print(\"\\nFeature Importance Analysis:\")\n",
    "# Get feature names from one-hot encoding\n",
    "cat_features = best_rf.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "all_features = np.concatenate([cat_features, numerical_features])\n",
    "\n",
    "# Extract and display top 20 features\n",
    "importances = best_rf.named_steps['classifier'].feature_importances_\n",
    "top_features = sorted(zip(all_features, importances), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"\\nTop 20 Predictive Features:\")\n",
    "for feature, importance in top_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# =============================================\n",
    "# END OF IMPROVED MODEL DEVELOPMENT\n",
    "# ============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results Summary\n",
    "\n",
    "Compare the accuracy and classification reports above. Random Forest and XGBoost often perform best for this type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dashboard Creation\n",
    "\n",
    "An interactive dashboard (Streamlit) visualizes the dataset, EDA results, and model performance. See instructions in the documentation section to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to data and plots\n",
    "PREPROCESSED_DATA_PATH = \"/home/ayyan/new_PROJECT/preprocessed_crime_data.parquet\"\n",
    "EDA_PLOTS_DIR = \"/home/ayyan/new_PROJECT/eda_plots\"\n",
    "\n",
    "st.set_page_config(layout=\"wide\", page_title=\"Crime Data Analysis Dashboard\")\n",
    "\n",
    "st.title(\"Crime Data Analysis Dashboard\")\n",
    "\n",
    "@st.cache_data\n",
    "def load_data(path):\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "# Load data\n",
    "df = load_data(PREPROCESSED_DATA_PATH)\n",
    "\n",
    "# Sidebar for navigation\n",
    "st.sidebar.title(\"Navigation\")\n",
    "page = st.sidebar.radio(\"Go to\", [\"Dataset Overview\", \"EDA Visualizations\", \"Model Performance\"]) # Model Performance will be empty for now\n",
    "\n",
    "if page == \"Dataset Overview\":\n",
    "    st.header(\"Dataset Overview\")\n",
    "    st.write(\"A quick look at the preprocessed crime data.\")\n",
    "\n",
    "    st.subheader(\"Data Sample\")\n",
    "    st.dataframe(df.head())\n",
    "\n",
    "    st.subheader(\"Dataset Shape\")\n",
    "    st.write(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "    st.subheader(\"Column Information\")\n",
    "    st.write(df.info())\n",
    "\n",
    "    st.subheader(\"Missing Values (after preprocessing)\")\n",
    "    st.write(df.isnull().sum())\n",
    "\n",
    "    st.subheader(\"Descriptive Statistics\")\n",
    "    st.write(df.describe())\n",
    "\n",
    "elif page == \"EDA Visualizations\":\n",
    "    st.header(\"Exploratory Data Analysis Visualizations\")\n",
    "    st.write(\"Visual insights into the crime data.\")\n",
    "\n",
    "    plot_files = [\n",
    "        \"victim_age_distribution.png\",\n",
    "        \"crimes_by_area.png\",\n",
    "        \"crimes_by_day_of_week.png\",\n",
    "        \"crimes_by_month.png\",\n",
    "        \"crimes_by_hour.png\",\n",
    "        \"correlation_heatmap.png\"\n",
    "    ]\n",
    "\n",
    "    for plot_file in plot_files:\n",
    "        plot_path = os.path.join(EDA_PLOTS_DIR, plot_file)\n",
    "        if os.path.exists(plot_path):\n",
    "            st.subheader(plot_file.replace(\"_\", \" \").replace(\".png\", \"\").title())\n",
    "            image = Image.open(plot_path)\n",
    "            st.image(image, use_column_width=True)\n",
    "        else:\n",
    "            st.warning(f\"Plot not found: {plot_file}\")\n",
    "\n",
    "elif page == \"Model Performance\":\n",
    "    st.header(\"Model Performance\")\n",
    "    st.write(\"This section will display the performance metrics of the trained models.\")\n",
    "    st.info(\"Model training and evaluation results will be displayed here once the model development section is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Documentation\n",
    "\n",
    "This section details the project documentation, including a `README.md` file and detailed explanations within this Jupyter notebook.\n",
    "\n",
    "### README.md Content\n",
    "\n",
    "```markdown\n",
    "# Crime Data Analysis Project\n",
    "\n",
    "## Project Overview\n",
    "This project performs a comprehensive data analysis on crime data from 2020 to present. It covers data understanding, preprocessing, exploratory data analysis (EDA), model development for crime type prediction, and an interactive dashboard for visualization.\n",
    "\n",
    "## Dataset Description\n",
    "The dataset `Crime_Data_from_2020_to_Present.csv` contains crime incidents recorded from 2020 onwards. Key features include crime type, location, time, victim demographics, and weapon information.\n",
    "\n",
    "## Installation Instructions\n",
    "1.  **Clone the repository:**\n",
    "    ```bash\n",
    "    git clone <repository_url>\n",
    "    cd <repository_name>\n",
    "    ```\n",
    "2.  **Create and activate a virtual environment:**\n",
    "    ```bash\n",
    "    python3 -m venv .venv\n",
    "    source .venv/bin/activate\n",
    "    ```\n",
    "3.  **Install dependencies:**\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "\n",
    "## Usage Guide\n",
    "1.  **Run the Jupyter Notebook:**\n",
    "    Open `crime_data_analysis.ipynb` in Jupyter Lab or Jupyter Notebook and run all cells sequentially to perform data processing, EDA, and model training.\n",
    "    ```bash\n",
    "    jupyter lab\n",
    "    # or\n",
    "    jupyter notebook\n",
    "    ```\n",
    "2.  **Run the Dashboard:**\n",
    "    After running the notebook and generating the necessary processed data and EDA plots, you can launch the interactive dashboard:\n",
    "    ```bash\n",
    "    streamlit run dashboard_app.py\n",
    "    ```\n",
    "\n",
    "## Results Summary\n",
    "(This section will be updated with key findings from EDA and model performance metrics after the notebook is fully executed.)\n",
    "\n",
    "## Future Improvements\n",
    "- Implement more advanced feature engineering (e.g., geospatial features).\n",
    "- Explore deep learning models for crime prediction.\n",
    "- Integrate real-time data streaming for live updates.\n",
    "- Enhance dashboard interactivity and add more detailed model insights.\n",
    "- Implement model explainability techniques (SHAP, LIME).\n",
    "- Create deployment-ready code using Flask/FastAPI.\n",
    "- Add unit tests for critical functions.\n",
    "- Include Docker setup for reproducibility.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus Features\n",
    "\n",
    "This section outlines potential bonus features that can be implemented to further enhance the project.\n",
    "\n",
    "### Model Explainability (SHAP, LIME)\n",
    "Implement techniques like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) to understand how individual features contribute to model predictions. This provides transparency and interpretability to the black-box models.\n",
    "\n",
    "### Deployment-Ready Code (Flask/FastAPI)\n",
    "Develop a simple API using Flask or FastAPI to serve the trained model. This would allow the model to be integrated into other applications or services for real-time predictions.\n",
    "\n",
    "### Unit Tests for Critical Functions\n",
    "Write unit tests for key functions in data processing, feature engineering, and model training to ensure code reliability and maintainability.\n",
    "\n",
    "### Docker Setup for Reproducibility\n",
    "Create a `Dockerfile` to containerize the entire project, including dependencies and the application. This ensures that the project can be easily reproduced and deployed across different environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_venv",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
